{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Aula 1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "zX4lQxMfHsNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Aula 1 - Spark Fundamentos"
      ]
    },
    {
      "metadata": {
        "id": "a1njSnn1Ge6B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuração de ambiente"
      ]
    },
    {
      "metadata": {
        "id": "drxkkwu7t0fd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b1af6731-1f90-488d-a478-8d9e286a1fa0"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 215.7MB 75kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 204kB 27.3MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3gJ1ZR_0t1zL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wW-mYvucH3Ob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's code!"
      ]
    },
    {
      "metadata": {
        "id": "VHixNvuOHL9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de tudo instalado, finalmente está na hora da diversão! Irei executar alguns comandos no Google Colab. Para entender todas as possibilidades que você tem, basta checar a documentação da API do PySpark em [6]. Primeiro, é necessário importar a API que será utilizada. Para esse exemplo, irei importar tudo da API pyspark.sql.\n",
        "\n",
        "from pyspark.sql import *\n",
        "\n",
        "Com esse simples comando, todas as funções do pyspark.sql estão disponíveis para uso. Iremos utilizar uma base de exemplos disponíveis dentro do próprio Google Colab. Para ler a base, basta executar o comando abaixo. Para mais informações referente a base, consulte [7]."
      ]
    },
    {
      "metadata": {
        "id": "DkrtteiNufMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Iniciando uma sessão no Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCIgm09yHIi5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Após a leitura, executei também o comando dataset.columns, que irá apresentar as colunas disponíveis no dataframe. "
      ]
    },
    {
      "metadata": {
        "id": "8-NNszKiuhhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ed44d07-bdda-41db-94ce-9815522bfd85"
      },
      "cell_type": "code",
      "source": [
        "#Importando um arquivo json\n",
        "dataset = spark.read.format(\"json\").option(\"multiLine\",True).load(\"sample_data/anscombe.json\")\n",
        "dataset.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Series', 'X', 'Y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "BMLpXdt6HUbr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "É possível você usar o comando show(), para verificar os valores. Conforme abaixo"
      ]
    },
    {
      "metadata": {
        "id": "F0BXSxymulqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "721ea7ca-5e83-4b0e-de6b-3b072bb45772"
      },
      "cell_type": "code",
      "source": [
        "#Vendo os 10 primeiros itens do nosso dataset\n",
        "dataset.show(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+\n",
            "|Series|   X|    Y|\n",
            "+------+----+-----+\n",
            "|     I|10.0| 8.04|\n",
            "|     I| 8.0| 6.95|\n",
            "|     I|13.0| 7.58|\n",
            "|     I| 9.0| 8.81|\n",
            "|     I|11.0| 8.33|\n",
            "|     I|14.0| 9.96|\n",
            "|     I| 6.0| 7.24|\n",
            "|     I| 4.0| 4.26|\n",
            "|     I|12.0|10.84|\n",
            "|     I| 7.0| 4.81|\n",
            "+------+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OAafzP9MHFbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que carregamos nossa base, verificamos que X e Y são possivelmente valores contínuos. Series é um campo categórico. Para isso, iremos usar o comando dtypes, para verificar as colunas."
      ]
    },
    {
      "metadata": {
        "id": "NniRcKVGum5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dc83e434-0134-41cc-fa5c-a17fed09caf9"
      },
      "cell_type": "code",
      "source": [
        "#Verificando o Schema \n",
        "dataset.printSchema()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Series: string (nullable = true)\n",
            " |-- X: double (nullable = true)\n",
            " |-- Y: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSKZNeR1HCHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ótimo, agora temos certeza que X e Y são valores contínuos e que Series é categórico. Por fim, vamos realizar um agrupamento na coluna Series e fazer a média dos valores de X e Y. Conforme descrição da base, ela contém 4 diferentes datasets com basicamente os mesmos valores de estatística descritiva. Será que conseguimos encontrar isso?"
      ]
    },
    {
      "metadata": {
        "id": "4F8QBf8yuoxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "820f9810-d59b-40f7-cb3b-6b4a5e869d24"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "dataset_agrupado = dataset.groupBy(\"Series\") \\\n",
        "                          .agg(F.avg(\"X\").alias(\"X_agrupado\"), F.avg(\"Y\").alias(\"Y_agrupado\"))  \\\n",
        "                          .orderBy(\"Series\")\n",
        "\n",
        "dataset_agrupado.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+-----------------+\n",
            "|Series|X_agrupado|       Y_agrupado|\n",
            "+------+----------+-----------------+\n",
            "|     I|       9.0|              7.5|\n",
            "|    II|       9.0|7.500909090909091|\n",
            "|   III|       9.0|7.500000000000001|\n",
            "|    IV|       9.0| 7.50090909090909|\n",
            "+------+----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H0s1RKmeHZrj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Excelente! O resultado é exatamente o esperado. Agora, vamos dar uma olhada nesse código mais a fundo. Na primeira linha eu importo novamente a API de funções com o alias F. Essa é uma das formas de se realizar o procedimento. Eu poderia simplesmente ter escrito “functions.avg()” no lugar de “F.avg()”, porém, prefiro usar o alias “F” nos meus códigos.\n",
        "\n",
        "Na segunda linha é definido todo o processamento em Spark para como se obter o valor agrupado de X e Y. Primeiro eu realizo um “groupBy”, que é uma função agregadora. Basicamente o que essa função faz é pegar os valores distintos da coluna “Séries” e usar como agrupador para os próximos comandos. Na segunda linha eu acho o comando .agg() é utilizado para realizar operação de agregação. F.avg é a função utilizada para calcular a média de um determinado campo e, por fim, .alias() é utilizado para definir o nome do campo.\n",
        "\n",
        "Se você executar esse comando sem o .show(), verá que ele é instantâneo. Isso não significa que o Spark já processou tudo e tem tudo calculado na memória, na verdade, isso apenas significa que ele montou o pipeline de execução para obter o valor que você deseja visualizar. Para que ele efetivamente processe o comando, você precisa usar uma ação, no nosso caso, um .show().\n",
        "\n",
        "Para dar uma pequena espiada em como o Spark funciona, execute o comando abaixo."
      ]
    },
    {
      "metadata": {
        "id": "_FfVmrZVuqjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "8b945eb4-b25e-402b-cf54-465d1272d7ac"
      },
      "cell_type": "code",
      "source": [
        "dataset_agrupado.explain()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(3) Sort [Series#0 ASC NULLS FIRST], true, 0\n",
            "+- Exchange rangepartitioning(Series#0 ASC NULLS FIRST, 200)\n",
            "   +- *(2) HashAggregate(keys=[Series#0], functions=[avg(X#1), avg(Y#2)])\n",
            "      +- Exchange hashpartitioning(Series#0, 200)\n",
            "         +- *(1) HashAggregate(keys=[Series#0], functions=[partial_avg(X#1), partial_avg(Y#2)])\n",
            "            +- *(1) FileScan json [Series#0,X#1,Y#2] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/content/sample_data/anscombe.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Series:string,X:double,Y:double>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}